{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSgmmOqh47Uz"
   },
   "source": [
    "**Professor:** Enrique Garcia Ceja\n",
    "**email:** enrique.gc@tec.mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88aPyKetABUv"
   },
   "source": [
    "### Team members that contributed to this activity (name and id):\n",
    "\n",
    "Member 1  \n",
    "Member 2  \n",
    "Member 3  \n",
    "Member 4  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DmWJzN87dC7"
   },
   "source": [
    "# Exercise: Detecting hand gestures from muscle electrical activity with an Ensemble of Neural Networks.\n",
    "\n",
    "In this exercise you will train your first deep neural network to detect hand gestures from muscle activity. In fact, you will build two deep neural networks and ensemble them.\n",
    "\n",
    "The data was collected with a MYO armband electromyography (EMG) sensor (see image below). The data was made available by Kirill Yashuk and can be downloaded [here](https://www.kaggle.com/kyr7plus/emg-4). The armband has 8 sensors that measure electrical activity at a sampling rate of 200Hz.\n",
    "The dataset contains 4 different gestures: <font color=blue>0-rock, 1-scissors, 2-paper, 3-OK.</font>\n",
    "The data contains 65 columns. The last column is the class label from 0 to 3. The first 64 columns are electrical measurements. 8 consecutive readings for each of the 8 sensors. The objective is to use the first 64 variables to predict the class.\n",
    "\n",
    "<table><tr><td><img src=\"https://github.com/enriquegit/ap-img/blob/main/img/myoband.jpg?raw=true\" width=\"200\"></td><td><img src=\"https://github.com/enriquegit/ap-img/blob/main/img/dnn.png?raw=true\" width=\"200\"></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FnXOSjX7dC9"
   },
   "source": [
    "## Instructions\n",
    "\n",
    "In this exercise you will train *two* neural networks and combine their results to produce the final predictions. During training time, you train the two networks with the same train data but each network should have a different architecture. There is no point of building the same two networks because they will produce the same predictions. At test time, a given instance is fed into both networks. Each network outputs the predicted probabilities for each class (by specifying the last activation to be softmax). One way to combine the predictions of both networks is to multiply the output probabilities and predict the class with the highest one. When more than two models compose the ensemble, majority voting can also be used.\n",
    "\n",
    "1. First, you need to randomly split the dataset into several subsets to avoid overfitting. Specifically, you will need **4 subsets**:\n",
    "\n",
    "<table><tr><td><img src=\"https://github.com/enriquegit/ap-img/blob/main/img/splits.png?raw=true\" width=\"250\"></td></tr></table>\n",
    "\n",
    "  - **train set (60%):** This one is used to train the two models.\n",
    "  - **val1 set (10%):** This is used to fine tune parameters of your nueral networks.\n",
    "  - **val2 set (15%):** This one is used to validate the performance when combining the networks.\n",
    "  - **test set (15%):** This one is used at the end only once to test the generalization performance of your model once you are happy with the performance on the *val2 set*.\n",
    "\n",
    "2. Build and train a first neural network using the *train set*. Use the *val1 set* to estimate its performance and fine tune its parameters.\n",
    "\n",
    "3. Build and train a second neural network using the *train set*. Use the *val1 set* to estimate its performance while fine tuning its parameters. This network needs to have a different architecture from the previous one.\n",
    "\n",
    "4. Once you are happy with both networks, it is now time to combine them. Generate predictions on the *val2 set* with your two networks. The predictions should be the probabilities for each class (not the final class). Combine the probabilities of each network by multiplying them. Obtain the final predicted classes by selecting the class with the highest probability.\n",
    "\n",
    "5. Evaluate the performance of the combined models and of each of the individual models on the same *val2 set*. The performance of the combined models should be better than the performance of the best individual model. If this is not the case, iterate from step 2.\n",
    "\n",
    "6. Once you are happy with your results, evaluate the performance of the combined models and each individual one on the *test set* just once.\n",
    "\n",
    "### Tips:\n",
    "\n",
    "- You can pass your validation data to the `fit` function with the `validation_data` argument, e.g., `model.fit(..., validation_data = (val1_features, val1_labels))`.\n",
    "- You can plot accuracy and loss curves to analyze your models behavior. See *demo_sports* notebook for an example.\n",
    "\n",
    "*NOTE:* In this case it was suggested to train both models with the same train data. With *Bagging* [1], which is an ensemble learning method, instead of using the same train data, new train sets are generated with bootstrapping. To construct a dataset, the method samples $N$ data points from the original train set where $N$ is the number of elements on the original train set. The sampling is made *with replacement*. Thus, the new set will contain duplicates and some elements of the original data set will not be present. The purpose of this is to have different models. If the same data is used to train them, they will be very similar and there is no need to train several of them. It is left as an exercise for the reader to implement bootstraping instead of using the same train set.\n",
    "\n",
    "[1] Breiman, L. (1996). Bagging predictors. Machine learning, 24(2), 123-140."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXuXqJZm7dC-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8reVPhH7dC-"
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "dataset = pd.concat(\n",
    "    map(pd.read_csv, ['datasets/0.csv', 'datasets/1.csv', 'datasets/2.csv', 'datasets/3.csv']), ignore_index=True)\n",
    "\n",
    "seed = 123 #set seed for reproducibility\n",
    "np.random.seed(seed)\n",
    "dataset = shuffle(dataset) #shuffle rows\n",
    "\n",
    "# Since there are not too many missing values we will just drop rows that contain missing values.\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# Print the dataset size after removing rows with missing values.\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpERU4rK7dC_"
   },
   "outputs": [],
   "source": [
    "# Print first rows of data.\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFvPtLie7dC_"
   },
   "outputs": [],
   "source": [
    "# Convert features and class to numpy arrays.\n",
    "features = dataset.drop('label', axis=1)\n",
    "\n",
    "labels = dataset[['label']]\n",
    "\n",
    "features = features.values\n",
    "\n",
    "labels = labels.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMGKQOc57dC_"
   },
   "source": [
    "Neural networks need the labels to be one-hot encoded. Currently, our labels are stored as strings. First we need to convert them into integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CI68ydJ97dC_"
   },
   "outputs": [],
   "source": [
    "# Convert labels to integers and store the result in labels_int\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "labels_int = le.fit_transform(labels.ravel())\n",
    "\n",
    "# Display first 5 labels as strings.\n",
    "print(labels[0:5])\n",
    "\n",
    "# Display first 5 labels as integers.\n",
    "print(labels_int[0:5]) # display first labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrjwjNpz7dC_"
   },
   "outputs": [],
   "source": [
    "# One hot encode the labels and store the result in a variable called 'labels'\n",
    "# You can use tf.keras.utils.to_categorical() function. Its first argument is an array of ints (e.g., labels_int)\n",
    "# The second argument is the number of classes.\n",
    "\n",
    "labels = tf.keras.utils.to_categorical(labels_int, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmoEMgPq7dDA"
   },
   "outputs": [],
   "source": [
    "# Print first five one-hot encoded labels\n",
    "\n",
    "labels[0:5,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ov76TaYu7dDA"
   },
   "source": [
    "## Step 1: Split the data.\n",
    "\n",
    "As explained in step 1 from the instructions section, the *train set* (60%) should consist of aprox. 6978 instances. The *val1 set* (10%) should have aprox. 1163 instances. The *val2 set* (15%) and the *test set* (15%) should have aprox. 1744 instances each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nVA6D61e7dDA"
   },
   "outputs": [],
   "source": [
    "# Currently, the features are stored in the 'features' variable and the labels in 'labels'.\n",
    "# First split into 2 subsets. train set = 60% (6978 instances) and a temporal set that contains the rest of the data (40%).\n",
    "# We will use the train_test_split() function.\n",
    "# The train_size argument specifies the number of instances to be included in the train set.\n",
    "\n",
    "train_features, tmp_features, train_labels, tmp_labels = train_test_split(features, labels,\n",
    "                                                                            train_size = 6978, random_state=1234)\n",
    "\n",
    "# Now train_features and train_labels contain the train set.\n",
    "# tmp_features and tmp_labels contain the rest of the data.\n",
    "\n",
    "# Let's print the shape of the train set. It should now contain 6978 instances.\n",
    "print(train_features.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CkMYFvr77dDA"
   },
   "outputs": [],
   "source": [
    "# Now, tmp_features has 40% of the total original data.\n",
    "# We need to split tmp_features and tmp_labels into the remaining test, val1 and val2 sets.\n",
    "# We can start by spliting tmp_features into two subsets.\n",
    "# The first subset correspons to the 10% of the total (1163 instances) of the val1 data.\n",
    "# The second subset is the remaining 30% of the total which will be split later into 15% for test and 15% for val2.\n",
    "\n",
    "# Use the train_test_split() function to split tmp_features and tmp_labels.\n",
    "# Store 10% of the TOTAL dataset into val1_features, val1_labels and the remaining into tmp2_features and tmp2_labels.\n",
    "\n",
    "#### YOUR CODE HERE ####\n",
    "\n",
    "\n",
    "# Print size of val1 set. It should have 1163 instances.\n",
    "print(val1_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7V5u8j1t7dDA"
   },
   "outputs": [],
   "source": [
    "# Now, tmp2_features, tmp2_labels contains 30% of the TOTAL dataset.\n",
    "# Split tmp2_features, tmp2_labels into two equally sized datasets.\n",
    "# test_features, test_labels and val2_features, val2_labels.\n",
    "\n",
    "#### YOUR CODE HERE ####\n",
    "\n",
    "\n",
    "\n",
    "# Print test and val2 sizes. Their sizes should be aprox. 1744.\n",
    "print(test_labels.shape)\n",
    "print(val2_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ooLUvZh7dDA"
   },
   "source": [
    "Noramlize the features between 0 and 1. Remember that normalization parameters are learned just from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56NJuAH_7dDA"
   },
   "outputs": [],
   "source": [
    "# Normalize features between 0 and 1.\n",
    "# Remember that normalization parameters are learned just from the training data.\n",
    "\n",
    "# Learn parameters from train set.\n",
    "normalizer = preprocessing.MinMaxScaler().fit(train_features)\n",
    "\n",
    "# Use the learned normalizer to normalize train_features and store the result in train_normalized.\n",
    "train_normalized = normalizer.transform(train_features)\n",
    "\n",
    "# Use the learned normalizer to normalize test_features and store the result in test_normalized.\n",
    "#### YOUR CODE HERE ####\n",
    "\n",
    "\n",
    "# Use the learned normalizer to normalize val1_features and store the result in val1_normalized.\n",
    "#### YOUR CODE HERE ####\n",
    "\n",
    "\n",
    "# Use the learned normalizer to normalize val2_features set and store the result in val2_normalized.\n",
    "#### YOUR CODE HERE ####\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtEB-5xS7dDA"
   },
   "source": [
    "# Step 2: Build and evaluate model 1.\n",
    "\n",
    "Now it is time to build model 1 (the first neural network). Use the keras Sequential API to build a deep network (at least 2 hidden layers). Store the model in a variable `model1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rejlkNrm7dDA"
   },
   "outputs": [],
   "source": [
    "# Define your neural network's architecture (layers, neurons, etc.).\n",
    "# The first Dense layer should include the input_shape, i.e., the number of input variables.\n",
    "# The following layers do not need an input_shape argument.\n",
    "# Since this is a classificaton problem, the last layer should have softmax as activation function.\n",
    "# keras.Sequential() expects a list of layers, e.g., keras.layers.Dense() which is a fully connected layer.\n",
    "# The number of neurons of a dense layer can be specified with the 'units' argument.\n",
    "# The activation function is specified with the 'activation' argument.\n",
    "\n",
    "#### COMPLETE THE CODE ####\n",
    "model1 = keras.Sequential([\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "# Print the model summary.\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIg9ImaK7dDA"
   },
   "source": [
    "### Define optimizer and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fIzn-QvD7dDA"
   },
   "outputs": [],
   "source": [
    "# We can use for example, Stochastic Gradient Descent as the optimizer.\n",
    "\n",
    "# Set a learning rate for the opimizer.\n",
    "lr =  #### COMPLETE THE CODE ####\n",
    "\n",
    "# Instantiate the optimzier with the specified learning rate.\n",
    "optimizer = tf.keras.optimizers.SGD(lr)\n",
    "\n",
    "# Compile the model.\n",
    "# Since this is a classification problem we need \"categorical_crossentropy\" as the loss function.\n",
    "model1.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tgspdbri7dDB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model with the normalized train set using the fit() function.\n",
    "# Use the normalized val1 set as validation data. You can do so with the 'validation_data' argument of the fit() function\n",
    "# that accepts a tuple as argument: model1.fit(..., validation_data = (val1_normalized, val1_labels))\n",
    "# Remember to set the number of epochs and the batch_size.\n",
    "\n",
    "# NOTE: After fitting a model its state is saved. If you change hyperparameters (learning rate, epohcs, etc.)\n",
    "# you should reinstantiate the model for example by re-reunning: model1 = keras.Sequential([....]]\n",
    "# compile the model again and fit.\n",
    "\n",
    "#### COMPLETE THE CODE ####\n",
    "history = model1.fit(\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YoDdp6T_7dDB",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the accuracy and loss curves.\n",
    "# Based on the validation accuracy and curves, fine tune your model,\n",
    "# for example by changing the number of epochs, learning rate, network architecture, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1CiGTaA7dDB"
   },
   "source": [
    "**Evaluate the performance on *val1 set*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wh9hcLId7dDB"
   },
   "outputs": [],
   "source": [
    "# The evaluate() function can be used to evaluate the model on a specified dataset.\n",
    "# It returns the loss and the metrics specified in fit(), in this case 'accuracy'.\n",
    "# The following code prints the final loss and accuracy and should be the same as printed in the last epoch.\n",
    "model1.evaluate(val1_normalized, val1_labels) #[loss, accuracy]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzUqXmN57dDB"
   },
   "source": [
    "# Step 3: build and evaluate model 2.\n",
    "\n",
    "Build another network. This one should have a different architecture from `model1`.\n",
    "Store the model in a variable `model2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRojl6J37dDB"
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE ####\n",
    "model2 = keras.Sequential([\n",
    "\n",
    "])\n",
    "\n",
    "# Print model summary.\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hw-49fFX7dDB"
   },
   "outputs": [],
   "source": [
    "# Instantiate an optimizer and specify the learning rate.\n",
    "\n",
    "#### YOUR CODE HERE ####\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model with the instantiated optimizer.\n",
    "#### YOUR CODE HERE ####\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQADUiPh7dDB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model with the normalized train set. Remember to set the number of epochs,\n",
    "# the validation_data and batch_size.\n",
    "\n",
    "#### COMPLETE THE CODE ####\n",
    "history2 = model2.fit(\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xUjQyBKE7dDB",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the accuracy and loss curves.\n",
    "# Based on the curves, fine tune your model, for example by changing the number of epochs, learning rate, network architecture, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# summarize history for accuracy\n",
    "plt.plot(history2.history['accuracy'])\n",
    "plt.plot(history2.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history2.history['loss'])\n",
    "plt.plot(history2.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_M0BmhW7dDB"
   },
   "source": [
    "**Evaluate the performance on *val1 set*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTKsniWS7dDB"
   },
   "outputs": [],
   "source": [
    "# Evaluate model2 using the evaluate() function.\n",
    "\n",
    "#### YOUR CODE HERE ####\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4j485u_Q7dDB"
   },
   "source": [
    "# Step 4 and 5: Combine models and evaluate the performance on *val2 set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNUHWCR47dDB"
   },
   "outputs": [],
   "source": [
    "# To combine model1 and model2 we can obtain the actual predictions from each model using the predict() function.\n",
    "# Its first argument is the features as numpy array.\n",
    "# This function returns a vector of probabilities for each row.\n",
    "# Each probability represents the likelihood of the corresponding class.\n",
    "\n",
    "# Make predictions on the val2 set using model 1.\n",
    "predictions1 = model1.predict(val2_normalized)\n",
    "\n",
    "# Make predictions on the val2 set using model 2.\n",
    "predictions2 = model2.predict(val2_normalized)\n",
    "\n",
    "# Combine the predictions by multiplying the probabilities.\n",
    "predictionsCombined = predictions1 * predictions2\n",
    "\n",
    "# Print the combined predictions of the first 5 instances.\n",
    "predictionsCombined[0:5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O0kPpt147dDB"
   },
   "outputs": [],
   "source": [
    "# Get the column index with max probability to get the predictions in integer format.\n",
    "predictions_int = np.argmax(predictionsCombined, axis=1)\n",
    "\n",
    "# Since the ground truth labels are also one-hot encoded we need to\n",
    "# get the index of the maximum value to obtain the predictions in integer format.\n",
    "true_values_int = np.argmax(val2_labels, axis=1)\n",
    "\n",
    "# Convert back to strings\n",
    "predictions_str = le.inverse_transform(predictions_int)\n",
    "\n",
    "true_values_str = le.inverse_transform(true_values_int)\n",
    "\n",
    "# Accuracy\n",
    "print(accuracy_score(true_values_str, predictions_str))\n",
    "\n",
    "# Recall\n",
    "print(recall_score(true_values_str, predictions_str, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uN-fA61-7dDB"
   },
   "source": [
    "### Evaluate performance of model 1 with *val2 set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nge4db047dDB"
   },
   "outputs": [],
   "source": [
    "# Compute the accuracy and recall just for model1.\n",
    "\n",
    "#### YOUR CODE HERE ####\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmBH6Hje7dDB"
   },
   "source": [
    "### Evaluate performance of model 2 with *val2 set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9VY-bMPN7dDB"
   },
   "outputs": [],
   "source": [
    "# Compute the accuracy and recall just for model2.\n",
    "\n",
    "#### YOUR CODE HERE ####\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8lp8AsY7dDC"
   },
   "source": [
    "Was the performance of the combined models better than the other two models?\n",
    "If yes, proceed to evaluate your models with the *test set*. If not, iterate from step 2-5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsKs7pM87dDH"
   },
   "source": [
    "# Step 6: Evaluate on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcElyOiy7dDH"
   },
   "source": [
    "### Combined models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "na46n7f77dDH"
   },
   "outputs": [],
   "source": [
    "# Evaluate the accuracy and recall of the combined models on the test set.\n",
    "\n",
    "#### YOUR CODE HERE ####\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ys22FTtE7dDH"
   },
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2DU_u6l7dDH"
   },
   "outputs": [],
   "source": [
    "# Evaluate the accuracy and recall of model1 on the test set.\n",
    "\n",
    "#### YOUR CODE HERE ####\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpnGKSUx7dDH"
   },
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfnguAh-7dDH"
   },
   "outputs": [],
   "source": [
    "# Evaluate the accuracy and recall of model1 on the test set.\n",
    "\n",
    "#### YOUR CODE HERE ####\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzeJ_e8s7dDH"
   },
   "source": [
    "**This is the end of the exercise!**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mv-tec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
